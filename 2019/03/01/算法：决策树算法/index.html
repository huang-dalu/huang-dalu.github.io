<!DOCTYPE html>
<html lang="en">
  <head><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">
<meta name="referrer" content="same-origin">
<meta name="referrer" content="no-referrer">

<meta name="description" content="算法：决策树算法"/><meta name="keywords" content="算法, UX Caff" /><link rel="alternate" href="/atom.xml" title="UX Caff" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.11.0" />
<link rel="canonical" href="http://example.com/2019/03/01/算法：决策树算法/"/>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.11.0" />

<script>
  window.config = {"toc":true,"fancybox":false,"pjax":false,"latex":true};
</script>

    <title>算法：决策树算法 - UX Caff</title>
  <meta name="generator" content="Hexo 5.2.0"></head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">UX Caff</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/">
        <li class="mobile-menu-item">Home
          </li>
      </a><a href="/archives/">
        <li class="mobile-menu-item">Archives
          </li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories
          </li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags
          </li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="" class="logo">UX Caff</a>
</div>

<nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/">
            Home
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            Archives
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/categories/">
            Categories
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/tags/">
            Tags
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/about/">
            About
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content"><article class="post">
    <header class="post-header">
      <h1 class="post-title">算法：决策树算法
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-03-01
        </span></div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-text">决策树的工作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E6%9E%84%E9%80%A0%E5%92%8C%E5%89%AA%E6%9E%9D"><span class="toc-text">决策树的构造和剪枝</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84-3-%E7%A7%8D%E7%AE%97%E6%B3%95"><span class="toc-text">决策树的 3 种算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%EF%BC%88ID3-%E7%AE%97%E6%B3%95%EF%BC%89"><span class="toc-text">信息增益（ID3 算法）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87%EF%BC%88C4-5-%E7%AE%97%E6%B3%95%EF%BC%89"><span class="toc-text">信息增益率（C4.5 算法）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0%EF%BC%88Cart-%E7%AE%97%E6%B3%95%EF%BC%89"><span class="toc-text">基尼指数（Cart 算法）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Cart-%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB%E6%A0%91"><span class="toc-text">Cart 算法分类树</span></a></li><li class="toc-item toc-level-5"><a class="toc-link"><span class="toc-text"></span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Cart-%E7%AE%97%E6%B3%95%E5%9B%9E%E5%BD%92%E6%A0%91"><span class="toc-text">Cart 算法回归树</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#-1"><span class="toc-text"></span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Cart-%E7%AE%97%E6%B3%95%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%89%AA%E6%9E%9D"><span class="toc-text">Cart 算法决策树的剪枝</span></a></li></ol></li></ol></li></ol>
    </div>
  </div><div class="post-content"><p>关于决策树的一些内容。<a id="more"></a></p>
<h3 id="决策树的工作原理"><a href="#决策树的工作原理" class="headerlink" title="决策树的工作原理"></a>决策树的工作原理</h3><p>决策树是一种非参数的有监督学习方法。它能够从一系列有特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。</p>
<p>决策树算法的本质是一种图结构。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/613759/1587790873691-0c1f2dfb-1df4-489c-8d1c-ede6a54c951f.png#align=left&display=inline&height=355&margin=%5Bobject%20Object%5D&name=image.png&originHeight=852&originWidth=1792&size=986002&status=done&style=none&width=746" alt="image.png"><br><em>图来自：陈旸</em></p>
<p>例如根据上图所示，决策树要做的是根据“天气”、“温度”、“湿度”、“刮风” 4 个特征来判断，最后是否要去打篮球。在整个决策过程中，一直对特征进行提问，并判断出最后结果。最初的问题阶段叫做根节点，在得到结论前的每一个问题都是中间节点，而得到的每一个结论都叫做叶子节点。也就是：</p>
<ul>
<li><strong>根节点</strong>：就是树的最顶端，最开始的节点。没有进边，有出边。在上图中，“天气”就是一个根节点。</li>
<li><strong>内部节点</strong>：就是树中间节点。有进边也有出边，进边只有一条，出边可以有很多条，比如说“温度”、“湿度”、“刮风”；</li>
<li><strong>叶节点</strong>：就是树最底部的节点。有进边，没有出边。每个叶子节点都是一个类别标签。也就是决策结果。</li>
</ul>
<h3 id="决策树的构造和剪枝"><a href="#决策树的构造和剪枝" class="headerlink" title="决策树的构造和剪枝"></a>决策树的构造和剪枝</h3><p>决策树根据节点的选择、以及节点的多少来决定决策结果。所以对于决策树来说，最重要的 2 个问题：</p>
<ol>
<li>如何找出最佳根节点和最佳子节点？</li>
<li>什么时候停止并得到最佳目标状态。也就是什么时候找出叶节点，防止过拟合？</li>
</ol>
<p>这两个问题，也就是决策树的<strong>构造</strong>和<strong>剪枝</strong>。</p>
<p>剪枝是为了防止过拟合情况出现。过拟合的意思就是模型训练过度，不具备泛化能力。实际场景中，造成过拟合的原因之一就是在训练中样本量太小，决策树选择的特征属性过多，就会把样本特点，当成总体数据特点。<br><img src="https://cdn.nlark.com/yuque/0/2020/png/613759/1587794615382-0a737221-0139-448a-994f-e05c0aeaea22.png#align=left&display=inline&height=234&margin=%5Bobject%20Object%5D&name=image.png&originHeight=814&originWidth=2594&size=1730107&status=done&style=none&width=746" alt="image.png"><br><em>图来自：陈旸</em><br>_<br>上图第 1 种情况属于欠拟合状态，第 3 种情况属于过拟合状态，第 2 种属于模型泛化能力较好的状态。</p>
<p>另外，剪枝也分为<strong>预剪枝</strong>和<strong>后剪枝：</strong><br>**</p>
<ul>
<li><strong>预剪枝：</strong>在构造决策树过程中提前停止。例如：指定决策树深度最大为 5，那么训练出来决策树的高度就是 5。预剪枝主要是建立某些规则限制决策树的生长。降低了过拟合的风险，降低了建树的时间，但是有可能带来欠拟合问题。</li>
<li><strong>后剪枝</strong>：从训练集生成一棵决策树，然后自下向上对非叶子结点进行考察，若该结点对应的子树用叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。使用“留出法”进行评估。即预留一部分数据用作“验证集”（周志华《机器学习》有较为详细的说明）。</li>
</ul>
<h3 id="决策树的-3-种算法"><a href="#决策树的-3-种算法" class="headerlink" title="决策树的 3 种算法"></a>决策树的 3 种算法</h3><p>在构造决策树时，本质是基于信息纯度来构造。信息纯度可以用<strong>信息熵</strong>来衡量。信息熵是衡量信息的不确定度或者说不纯度。信息不确定度越大，信息熵越大。</p>
<p>信息纯度的理解，就是目标变量的分歧最小。也就是纯度越低，信息熵越高。举例说明：</p>
<p>场景一：4 次去打篮球，2 次不去打篮球；<br>场景二：3 次去打篮球，3 次不去打篮球。</p>
<p>以上两种场景下，场景一的信息纯度高于场景二。具体高多少，就用信息熵来对比衡量。信息熵公式：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/613759/1587801008853-707bb15a-97e9-4f1a-8fac-c93362f84ece.png#align=left&display=inline&height=108&margin=%5Bobject%20Object%5D&name=image.png&originHeight=108&originWidth=504&size=9016&status=done&style=none&width=504" alt="image.png"><br>在决策树算法中，基于信息纯度构造的常用算法有 3 种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。</p>
<h4 id="信息增益（ID3-算法）"><a href="#信息增益（ID3-算法）" class="headerlink" title="信息增益（ID3 算法）"></a>信息增益（ID3 算法）</h4><p>信息增益指的是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/613759/1587814040601-e01b6983-2c0a-43e4-adf6-ad7b0051d73d.png#align=left&display=inline&height=84&margin=%5Bobject%20Object%5D&name=image.png&originHeight=84&originWidth=513&size=9908&status=done&style=none&width=513" alt="image.png"><br>简单来说，信息熵越小，信息增益越大，样本纯度越高，选择信息增益最大的特征作为分裂标准。ID3 算法流程：</p>
<ol>
<li>自上而下，遍历所有的特征值；</li>
<li>计算每个节点信息增益，将信息增益最大的节点作为父节点；</li>
<li>根据分裂属性划分内部节点；</li>
<li>重复上述流程，直至满足条件结束。</li>
</ol>
<p>ID3 算法特点：</p>
<ol>
<li>有些属性可能对分类任务没有太大作用，但是仍然可能会被选为最优属性；</li>
<li>ID3 算法倾向于选择取值比较多的属性。例如：把“编号”作为一个属性，那么“编号”将会被选为最优属性；</li>
<li>不适用于连续变量，只能用于分类。</li>
</ol>
<h4 id="信息增益率（C4-5-算法）"><a href="#信息增益率（C4-5-算法）" class="headerlink" title="信息增益率（C4.5 算法）"></a>信息增益率（C4.5 算法）</h4><p>信息增益率（C4.5算法）是在 ID3 算法上进行改进的一种算法。主要改进点有：</p>
<ul>
<li>采用信息增益率。因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵。当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。</li>
</ul>
<ul>
<li>采用悲观剪枝。ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。</li>
</ul>
<ul>
<li>离散化处理连续属性。C4.5 可以对连续属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢。C4.5 选择具有最高信息增益的划分所对应的阈值。</li>
</ul>
<ul>
<li>处理缺失值。针对数据集不完整的情况，C4.5 也可以进行处理。这种算法会在最后使用已有数据样本值 / 总体值*信息增益率。</li>
</ul>
<h4 id="基尼指数（Cart-算法）"><a href="#基尼指数（Cart-算法）" class="headerlink" title="基尼指数（Cart 算法）"></a>基尼指数（Cart 算法）</h4><p>ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。并且，Cart 算法既可以做决策树分类，也可以用做回归。分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。</p>
<h5 id="Cart-算法分类树"><a href="#Cart-算法分类树" class="headerlink" title="Cart 算法分类树"></a>Cart 算法分类树</h5><p>ID3 是基于信息增益做判断，C4.5 在 ID3 的基础上做了改进，提出了信息增益率的概念。实际上 CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。</p>
<p>基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。GINI 系数的计算公式：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/613759/1587817712702-8ec6c09c-c5ba-458b-813f-a6797ef2192f.png#align=left&display=inline&height=94&margin=%5Bobject%20Object%5D&name=image.png&originHeight=94&originWidth=463&size=9331&status=done&style=none&width=463" alt="image.png"></p>
<p>基尼系数算法举例：</p>
<ul>
<li>场景一：6 个都去打篮球。所有人都去打篮球，所以 p(Ck|t) = 1，因此 GINI(t) = 1 - 1 = 0</li>
<li>场景二：3 个去打篮球，3 个不去打篮球。有一半人去打篮球，而另一半不去打篮球，所以，p(C1|t) = 0.5，p(C2|t) = 0.5，GINI(t) = 1 -（0.5 * 0.5 + 0.5 * 0.5）= 0.5</li>
</ul>
<p>通过两个基尼系数你可以看出，场景一基尼系数最小，也证明样本最稳定，而场景二样本不稳定性更大。<br>**<br><strong>案例说明</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">iris=load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特征集和分类标识</span></span><br><span class="line">features = iris.data</span><br><span class="line">labels = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机抽取33%的数据作为测试集，其余为训练集</span></span><br><span class="line">train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=<span class="number">0.33</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建CART分类树</span></span><br><span class="line">clf = DecisionTreeClassifier(criterion=<span class="string">&#x27;gini&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合构造CART分类树</span></span><br><span class="line">clf = clf.fit(train_features, train_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用CART分类树做预测</span></span><br><span class="line">test_predict = clf.predict(test_features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测结果与测试集结果作比对</span></span><br><span class="line">score = accuracy_score(test_labels, test_predict)</span><br><span class="line">print(<span class="string">&quot;CART分类树准确率 %.4lf&quot;</span> % score)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CART分类树准确率 <span class="number">0.9600</span></span><br></pre></td></tr></table></figure>
<h5 id=""><a href="#" class="headerlink" title=""></a></h5><h5 id="Cart-算法回归树"><a href="#Cart-算法回归树" class="headerlink" title="Cart 算法回归树"></a>Cart 算法回归树</h5><p>CART 回归树划分数据集的过程和分类树的过程一样，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在 CART 分类树中采用的是基尼系数作为标准，在 CART 回归树中，采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。</p>
<p>样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。假设 x 为样本的个体，均值为 u。为了统计样本的离散程度，可以取差值的绝对值，或者方差。</p>
<p>其中差值的绝对值为样本值减去样本均值的绝对值：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2020/png/613759/1587818428201-c86d6f00-d691-4c8b-8bd0-8c12a8f4b48a.png#align=left&display=inline&height=125&margin=%5Bobject%20Object%5D&name=image.png&originHeight=125&originWidth=208&size=3674&status=done&style=none&width=208" alt="image.png"></p>
<p>方差为每个样本值减去样本均值的平方和除以样本个数：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/613759/1587818438819-d7f2401a-70e4-4cbe-86f4-de124dc89321.png#align=left&display=inline&height=87&margin=%5Bobject%20Object%5D&name=image.png&originHeight=87&originWidth=245&size=4950&status=done&style=none&width=245" alt="image.png"></p>
<p>所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。</p>
<p><strong>案例说明</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score,mean_absolute_error,mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">boston=load_boston()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 探索数据</span></span><br><span class="line">print(boston.feature_names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特征集和房价</span></span><br><span class="line">features = boston.data</span><br><span class="line">prices = boston.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机抽取33%的数据作为测试集，其余为训练集</span></span><br><span class="line">train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=<span class="number">0.33</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建CART回归树</span></span><br><span class="line">dtr=DecisionTreeRegressor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合构造CART回归树</span></span><br><span class="line">dtr.fit(train_features, train_price)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测测试集中的房价</span></span><br><span class="line">predict_price = dtr.predict(test_features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集的结果评价；每次结果可能会有不同</span></span><br><span class="line">print(<span class="string">&#x27;回归树二乘偏差均值:&#x27;</span>, mean_squared_error(test_price, predict_price))</span><br><span class="line">print(<span class="string">&#x27;回归树绝对值偏差均值:&#x27;</span>, mean_absolute_error(test_price, predict_price)) </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;CRIM&#x27;</span> <span class="string">&#x27;ZN&#x27;</span> <span class="string">&#x27;INDUS&#x27;</span> <span class="string">&#x27;CHAS&#x27;</span> <span class="string">&#x27;NOX&#x27;</span> <span class="string">&#x27;RM&#x27;</span> <span class="string">&#x27;AGE&#x27;</span> <span class="string">&#x27;DIS&#x27;</span> <span class="string">&#x27;RAD&#x27;</span> <span class="string">&#x27;TAX&#x27;</span> <span class="string">&#x27;PTRATIO&#x27;</span> <span class="string">&#x27;B&#x27;</span> <span class="string">&#x27;LSTAT&#x27;</span>]</span><br><span class="line">回归树二乘偏差均值: <span class="number">23.80784431137724</span></span><br><span class="line">回归树绝对值偏差均值: <span class="number">3.040119760479042</span></span><br></pre></td></tr></table></figure>
<h5 id="-1"><a href="#-1" class="headerlink" title=""></a></h5><h5 id="Cart-算法决策树的剪枝"><a href="#Cart-算法决策树的剪枝" class="headerlink" title="Cart 算法决策树的剪枝"></a>Cart 算法决策树的剪枝</h5><p>CART 决策树的剪枝主要采用的是 CCP 方法，一种后剪枝的方法。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：<br><img src="https://cdn.nlark.com/yuque/0/2020/png/613759/1587819062815-f822b059-d6b4-4f24-978c-923d560a5961.png#align=left&display=inline&height=140&margin=%5Bobject%20Object%5D&name=image.png&originHeight=140&originWidth=327&size=7783&status=done&style=none&width=327" alt="image.png"><br>其中 Tt 代表以 t 为根节点的子树，C(Tt) 表示节点 t 的子树没被裁剪时子树 Tt 的误差，C(t) 表示节点 t 的子树被剪枝后节点 t 的误差，|Tt|代子树 Tt 的叶子数，剪枝后，T 的叶子数减少了|Tt|-1。</p>
<p>所以节点的表面误差率增益值等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。</p>
<p>因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。</p>

      </div>
      
      <footer class="post-footer">
        <div class="post-tags">
            <a href="/tags/%E7%AE%97%E6%B3%95/">算法</a>
            </div>
        
        <nav class="post-nav"><a class="prev" href="/2019/04/10/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0Bagging%E5%92%8CBoosting%E7%AE%80%E8%BF%B0/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">集成学习Bagging和Boosting简述</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    <a class="next" href="/2019/02/01/%E5%85%AC%E4%BC%97%E5%8F%B7%E6%8E%92%E7%89%88%E8%A7%84%E5%88%99%E8%AF%B4%E6%98%8E/">
        <span class="next-text nav-default">技术公众号排版规则说明</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments">
      <div id="vcomments"></div>
    </div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:huangdalu@yeah.net" class="iconfont icon-email" title="email"></a>
        <a target="_blank" rel="noopener" href="https://github.com/huang-dalu" class="iconfont icon-github" title="github"></a>
        <a target="_blank" rel="noopener" href="https://www.douban.com/people/108055759" class="iconfont icon-douban" title="douban"></a>
        <a href="/atom.xml" class="iconfont icon-rss" title="rss" target="_blank"></a>
    </div><div class="copyright"><span class="division">&nbsp;</span><span class="copyright-year">Since 2020 - 2021
      <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">huangdalu</span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
<script src="./lib/valine/av-min.js?v=3.0.4"></script>
<script src='./lib/valine/Valine.min.js'></script>
<script type="text/javascript">
    new Valine({
        el: '#vcomments',
        notify: false,
        verify: false,
        app_id: "XtxvAXPwOzM9noIc3eyxi3AS-gzGzoHsz",
        app_key: "yEVQszyxb4bwLuAGU5VHnPR8",
        placeholder: "说点什么吧...",
        avatar: 'mm',
        visitor: 'ture'
    });
</script><script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/js/src/even.js?v=2.11.0"></script>
</body>
</html>
